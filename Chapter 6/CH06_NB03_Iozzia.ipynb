{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro2mCsr9wNoZ"
      },
      "source": [
        "# Benchmarking Python Code Generation with Vanilla and 8-bit Quantized StarCoder2 Models\n",
        "This notebook is a companion of chapter 6 of the \"Domain Specific LLMs in Action\" book, author Guglielmo Iozzia, [Manning Publications](https://www.manning.com/), 2024.  \n",
        "The code in this notebook is to benchmark inference performance (latency and throughtput) when generating Python code using a vanilla [StarCoder2](https://huggingface.co/Salesforce/codegen-350M-mono) 2B model, and after 8-bit quantization of the same model. It reuqires hardware acceleration.  \n",
        "More details about the code can be found in the related book's chapter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxm-Oc-FxRGx"
      },
      "source": [
        "Install the missing requirements in the ColabVM (only HF's Optimum for the ONNX runtime and Bitsandbytes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBniEqbYvTsI"
      },
      "outputs": [],
      "source": [
        "!pip install optimum[onnxruntime-gpu]==1.21.2\n",
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpSD_2ERL_Q1"
      },
      "source": [
        "Upgrade the Numpy and HF's Transformers packages to the latest version. A restart of the VM is needed after."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0qslPrQC5QU"
      },
      "outputs": [],
      "source": [
        "!pip install -U numpy transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Qgo8j3yAEC_"
      },
      "source": [
        "### Vanilla Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3uieL3kxc-C"
      },
      "source": [
        "Download the StarCoder2-3B model (in bfloat16) and its tokenizer from the HF's Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQXJ5wJJuMUd"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_id = \"bigcode/starcoder2-3b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JpoSzoHaOR4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                             device_map='auto',\n",
        "                                             torch_dtype=torch.bfloat16)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlUHkMBOxtGL"
      },
      "source": [
        "Set a text prompt (a Python function header) to be used across benchmarks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Euq2A50ANLW"
      },
      "outputs": [],
      "source": [
        "prompt = \"def print_hello_world():\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QIHUGmFx1EB"
      },
      "source": [
        "The code in the following cell is just to verify that model and tokenizer have been downloaded properly. You can skip its execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_AXjHFvaTO8"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(inputs)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "td1Nl5uUx_6v"
      },
      "source": [
        "Setup a Transformers' pipeline for inference with the vanilla model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTBIqZ9MmF6P"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            do_sample=True,\n",
        "            use_cache=True,\n",
        "            temperature=0.2,\n",
        "            top_p=0.95,\n",
        "            max_length=14\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3x3oWkbyHFw"
      },
      "source": [
        "Test the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4NxE8a1m6xE"
      },
      "outputs": [],
      "source": [
        "result = pipe(prompt)\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvxNVcKjGgGS"
      },
      "source": [
        "Save the checkpoints locally, to be reused when quantizing it later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIIsBPsmudM3"
      },
      "outputs": [],
      "source": [
        "checkpoint_save_dir = 'local-pt-checkpoint'\n",
        "tokenizer.save_pretrained(checkpoint_save_dir)\n",
        "model.save_pretrained(checkpoint_save_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q70kfNsKCeYx"
      },
      "source": [
        "Define some utils for benchmarking (more details about them in chapter 6 of the book)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pL77v2MCEhNx"
      },
      "outputs": [],
      "source": [
        "from contextlib import contextmanager\n",
        "from dataclasses import dataclass\n",
        "from time import perf_counter\n",
        "\n",
        "@contextmanager\n",
        "def track_infer_time(time_buffer):\n",
        "    start_time = perf_counter()\n",
        "    yield\n",
        "    end_time = perf_counter()\n",
        "\n",
        "    time_buffer.append(end_time - start_time)\n",
        "\n",
        "@dataclass\n",
        "class BenchmarkInferenceResult:\n",
        "    model_inference_time: [int]\n",
        "    optimized_model_path: str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il5ctRs0yU7Y"
      },
      "source": [
        "Define a custom funtion to be reused across benchmarks with the different versions of the model under evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kROG4REwFO2d"
      },
      "outputs": [],
      "source": [
        "from tqdm import trange\n",
        "\n",
        "def benchmark_inference(providers_dict, pipe, prompt, results):\n",
        "  for device, label in PROVIDERS:\n",
        "      for _ in trange(10, desc=\"Warming up\"):\n",
        "          pipe(prompt)\n",
        "\n",
        "      time_buffer = []\n",
        "      for _ in trange(100, desc=f\"Tracking inference time ({label})\"):\n",
        "        with track_infer_time(time_buffer):\n",
        "            pipe(prompt)\n",
        "\n",
        "      results[label] = BenchmarkInferenceResult(\n",
        "          time_buffer,\n",
        "          None\n",
        "      )\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9-RNdkvyfQ1"
      },
      "source": [
        "Execute the benchmarks for the StarCoder2 vanilla model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL9m9HtcFsP9"
      },
      "outputs": [],
      "source": [
        "results = {}\n",
        "PROVIDERS = {\n",
        "    (\"gpu\", \"PyTorch GPU\"),\n",
        "}\n",
        "results = benchmark_inference(PROVIDERS, pipe, prompt, results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKlAIq_oz81N"
      },
      "source": [
        "### 8-bit Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KESvr_wOzy7i"
      },
      "source": [
        "To prevent potential out of memory issues, let's do some VRAM and RAM cleanup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQUAblExJWvm"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "model.cpu()\n",
        "del model\n",
        "del pipe\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nijBTdmv0V41"
      },
      "source": [
        "Do 8-bit quantization of the original model and save it to disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfM7NeIb2vqR"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint_save_dir)\n",
        "quantized_model = AutoModelForCausalLM.from_pretrained(checkpoint_save_dir,\n",
        "                                        quantization_config=quantization_config)\n",
        "quantized_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW11NW8NfpUY"
      },
      "source": [
        "The code in the following cell is just to verify that model and tokenizer have been downloaded properly. You can skip its execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evHyv8igwX64"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "outputs = quantized_model.generate(inputs)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klEhjGNPI4xk"
      },
      "outputs": [],
      "source": [
        "quantized_model.save_pretrained('local-8bit-checkpoint')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJDOQ80HKlWC"
      },
      "outputs": [],
      "source": [
        "checkpoint_8bit_save_dir = 'local-8bit-checkpoint'\n",
        "\n",
        "# Load the quantized model from the specified directory\n",
        "quantized_model_loaded = AutoModelForCausalLM.from_pretrained(checkpoint_8bit_save_dir)\n",
        "quantized_model_loaded.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUosoY7N0o3C"
      },
      "source": [
        "Setup the pipeline for inference with the quantized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GQypqF7Qco_"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\"text-generation\",\n",
        "            model=quantized_model_loaded,\n",
        "            tokenizer=tokenizer,\n",
        "            do_sample=True,\n",
        "            use_cache=True,\n",
        "            temperature=0.2,\n",
        "            top_p=0.95,\n",
        "            max_length=14,\n",
        "            #accelerator=\"ort\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW0GqH1E17dz"
      },
      "source": [
        "Verify that the pipeline works as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehBfVq_CQq28"
      },
      "outputs": [],
      "source": [
        "result = pipe(prompt)\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGUi3AaYDC-I"
      },
      "source": [
        "Repeat the benchmark on the quantized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4cTBDxVIu4X"
      },
      "outputs": [],
      "source": [
        "PROVIDERS = {\n",
        "    (\"ort\", \"Quant GPU\"),\n",
        "}\n",
        "results = benchmark_inference(PROVIDERS, pipe, prompt, results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56NkSjpODGv5"
      },
      "source": [
        "### Results of the Benchmarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ak1DWKV2Y5p"
      },
      "source": [
        "Visually compare the average inference times across benchmarks for the 2 different versions of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QmZNhc4DMhq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import plotly.express as px\n",
        "\n",
        "# Compute average inference time\n",
        "time_results = {k: np.mean(v.model_inference_time) * 1e3 for k, v in results.items()}\n",
        "\n",
        "fig = px.bar(x=time_results.keys(), y=time_results.values(),\n",
        "             title=\"Average inference time (ms) for each provider\",\n",
        "             labels={'x':'Provider', 'y':'Avg Inference time (ms)'},\n",
        "             text_auto='.2s')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgE7aBCb2l32"
      },
      "source": [
        "Calculate latency and throughput metrics for the 3 benchmark sets and put them into a Pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOsjKpmlK_PQ"
      },
      "outputs": [],
      "source": [
        "time_results = {k: np.mean(v.model_inference_time) * 1e3 for k, v in results.items()}\n",
        "time_results_std = {k: np.std(v.model_inference_time) * 1000 for k, v in results.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uao1a7ALLAP6"
      },
      "outputs": [],
      "source": [
        "perf_results = {}\n",
        "for k, v in results.items():\n",
        "  latency_list = v.model_inference_time\n",
        "  latency_50 = np.percentile(latency_list, 50) * 1e3\n",
        "  latency_75 = np.percentile(latency_list, 75) * 1e3\n",
        "  latency_90 = np.percentile(latency_list, 90) * 1e3\n",
        "  latency_95 = np.percentile(latency_list, 95) * 1e3\n",
        "  latency_99 = np.percentile(latency_list, 99) * 1e3\n",
        "\n",
        "  average_latency = np.mean(v.model_inference_time) * 1e3\n",
        "  throughput = 1 * (1000 / average_latency)\n",
        "\n",
        "  perf_results[k] = (\n",
        "        average_latency,\n",
        "        latency_50,\n",
        "        latency_75,\n",
        "        latency_90,\n",
        "        latency_95,\n",
        "        latency_99,\n",
        "        throughput,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5AMItfgMQMi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "index_labels = ['Average_latency (ms)', 'Latency_P50', 'Latency_P75',\n",
        "                'Latency_P90', 'Latency_P95', 'Latency_P99', 'Throughput']\n",
        "perf_df = pd.DataFrame(data=perf_results, index=index_labels)\n",
        "perf_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOFMzIya283h"
      },
      "source": [
        "Visually compare inference durations across benchmarks for the 2 different versions of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22IhNSXGPcfC"
      },
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame(columns=['Provider', 'Inference_time'])\n",
        "for k, v in results.items():\n",
        "  for i in range(len(v.model_inference_time)):\n",
        "    results_df.loc[len(results_df.index)] = [k, v.model_inference_time[i] * 1e3]\n",
        "\n",
        "fig = px.box(results_df, x=\"Provider\", y=\"Inference_time\",\n",
        "             points=\"all\",\n",
        "             labels={'Provider':'Provider', 'Inference_time':'Inference durations (ms)'})\n",
        "fig.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
