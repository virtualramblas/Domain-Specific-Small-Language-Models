{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization of the GPT-2 Small Model\n",
        "This notebook is a companion of chapter 5 of the \"Domain Specific LLMs in Action\" book, author Guglielmo Iozzia, [Manning Publications](https://www.manning.com/), 2024.  \n",
        "The code in this notebook is to introduce readers to the quantization of a decoder-only language model, [GPT-2 Small](https://huggingface.co/openai-community/gpt2). It doesn't require hardware acceleration.  \n",
        "More details about the code can be found in the related book's chapter."
      ],
      "metadata": {
        "id": "DCgYUlx0DB6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the required packages and classes."
      ],
      "metadata": {
        "id": "o7jiwZbrDcFS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvBTSJ2gSm5y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the GPT-2 Small model and associated tokenizer from the HF's Hub and load it to CPU. Finally print the size (in bytes) of the model in memory."
      ],
      "metadata": {
        "id": "1WUf_-2EDoUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cpu'\n",
        "\n",
        "model_id = 'openai-community/gpt2'\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "print(f\"Model size: {model.get_memory_footprint():,} bytes\")"
      ],
      "metadata": {
        "id": "L9SqaXHgS28V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a custom function to perform *absmax* quantization and dequantization."
      ],
      "metadata": {
        "id": "11wBCto6D4eS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def absmax_quantize(X):\n",
        "    # Calculate scale\n",
        "    scale = 127 / torch.max(torch.abs(X))\n",
        "\n",
        "    # Quantize\n",
        "    X_quant = (scale * X).round()\n",
        "\n",
        "    # Dequantize\n",
        "    X_dequant = X_quant / scale\n",
        "\n",
        "    return X_quant.to(torch.int8), X_dequant"
      ],
      "metadata": {
        "id": "BMQZDqLYaJPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone the source model and apply the previously defined quantization function to all the weights of the cloned copy."
      ],
      "metadata": {
        "id": "rWnUvMrlEdoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "\n",
        "weights = [param.data.clone() for param in model.parameters()]\n",
        "\n",
        "model_abs = deepcopy(model)\n",
        "\n",
        "weights_abs = []\n",
        "for param in model_abs.parameters():\n",
        "    _, dequantized = absmax_quantize(param.data)\n",
        "    param.data = dequantized\n",
        "    weights_abs.append(dequantized)"
      ],
      "metadata": {
        "id": "XPNDwVl_cOum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the matplotlib library, plot the distribution of the weights for the source model and the quantized version both on the same histogram chart."
      ],
      "metadata": {
        "id": "XJQFjswmE0A0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "weights = np.concatenate([t.cpu().numpy().flatten() for t in weights])\n",
        "weights_abs = np.concatenate([t.cpu().numpy().flatten() for t in weights_abs])"
      ],
      "metadata": {
        "id": "lt7DlQh-fpuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set background style\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "# Create figure and axes\n",
        "fig, axs = plt.subplots(1, figsize=(10,10), dpi=300, sharex=True)\n",
        "\n",
        "# Plot the histograms for original and zero-point weights\n",
        "axs.hist(weights, bins=150, alpha=0.5, label='Original weights', color='blue', range=(-2, 2))\n",
        "axs.hist(weights_abs, bins=150, alpha=0.5, label='Absmax weights', color='yellow', range=(-2, 2))\n",
        "\n",
        "# Add grid\n",
        "axs.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Add legend\n",
        "axs.legend()\n",
        "\n",
        "# Add title and labels\n",
        "axs.set_title('Comparison of Original and Absmax Quantized Weights', fontsize=16)\n",
        "\n",
        "axs.set_xlabel('Weights', fontsize=14)\n",
        "axs.set_ylabel('Count', fontsize=14)\n",
        "axs.yaxis.set_major_formatter(ticker.EngFormatter()) # Make y-ticks more human readable\n",
        "\n",
        "# Improve font\n",
        "plt.rc('font', size=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UoSDw5MfkPeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function to generate text, whatever the model (original or quantized)."
      ],
      "metadata": {
        "id": "Broep3T1FLtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, input_text, max_length=100):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "    output = model.generate(inputs=input_ids,\n",
        "                            max_length=max_length,\n",
        "                            do_sample=True,\n",
        "                            top_k=30,\n",
        "                            pad_token_id=tokenizer.eos_token_id,\n",
        "                            attention_mask=input_ids.new_ones(input_ids.shape))\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "ccynNm1wqCgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the text generation function defined in the previous code cell to generate text with both model versions (the original and the one after quantization)."
      ],
      "metadata": {
        "id": "E_TbqdZYFTWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'My favourite school subject is'\n",
        "original_text = generate_text(model, prompt)\n",
        "absmax_text   = generate_text(model_abs, prompt)\n",
        "\n",
        "print(f\"Original model:\\n{original_text}\")\n",
        "print(f\"Absmax model:\\n{absmax_text}\")"
      ],
      "metadata": {
        "id": "qy--hhbAsLXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function to calculate the perplexity score."
      ],
      "metadata": {
        "id": "a5TAMxp5Fivf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_perplexity(model, text, device):\n",
        "    encodings = tokenizer(text, return_tensors='pt').to(device)\n",
        "\n",
        "    input_ids = encodings.input_ids\n",
        "    target_ids = input_ids.clone()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=target_ids)\n",
        "\n",
        "    neg_log_likelihood = outputs.loss\n",
        "\n",
        "    perplexity = torch.exp(neg_log_likelihood)\n",
        "\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "iZABZBZ-4M1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the perplexity score for both versions of the model, using the text results previously generated by both."
      ],
      "metadata": {
        "id": "37l7xH9mFt2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity = calculate_perplexity(model, original_text, device)\n",
        "perplexity_absmax = calculate_perplexity(model_abs, absmax_text, device)\n",
        "\n",
        "print(f\"Original perplexity:  {perplexity.item():.2f}\")\n",
        "print(f\"Absmax perplexity:    {perplexity_absmax.item():.2f}\")"
      ],
      "metadata": {
        "id": "g_zDcCH3Eita"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}